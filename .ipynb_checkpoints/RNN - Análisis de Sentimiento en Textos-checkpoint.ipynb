{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de Sentimiento en Textos\n",
    "Alan Badillo Salas (badillo.soft@hotmail.com)\n",
    "\n",
    "En estre proyecto vamos a utilizar las redes neuronales de tipo RNN (Redes Reuronales Recurrentes), para poder predecir si un texto (una reseña en inglés) será positiva o negativa. Para ello necesitamos una base de entrenamiento la cuál fue recreada del artículo: https://medium.com/@dclengacher/keras-lstm-recurrent-neural-networks-c1f5febde03d.\n",
    "\n",
    "Podemos descargar el corpus directo en: https://github.com/zaidalyafeai/Browser-Sentiment-Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El punto de partida en nuestro análisis es un corpus que contiene `7086` reseñas obtenidas de IMDB y consisten en un breve texto en inglés y una etiqueta sobre si la reseña es positiva (`1`) o si es negativa (`0`).\n",
    "\n",
    "Vamos a cargar el corpus en pandas para analizarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7086 entries, 0 to 7085\n",
      "Data columns (total 2 columns):\n",
      "text     7086 non-null object\n",
      "label    7086 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 110.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "corpus = pd.read_csv(\"http://badillosoft.com/corpus.csv\")\n",
    "\n",
    "corpus = corpus.filter(items=[\"text\", \"label\"])\n",
    "\n",
    "print(corpus.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0            The Da Vinci Code book is just awesome.      1\n",
      "1  this was the first clive cussler i've ever rea...      1\n",
      "2                   i liked the Da Vinci Code a lot.      1\n",
      "3                   i liked the Da Vinci Code a lot.      1\n",
      "4  I liked the Da Vinci Code but it ultimatly did...      1\n"
     ]
    }
   ],
   "source": [
    "print(corpus.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para no perder generalidad vamos a revolver aleatoriamente el corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  label\n",
      "3479           Brokeback Mountain was an AWESOME movie.      1\n",
      "5535  I hate Harry Potter, that daniel wotshisface n...      0\n",
      "3539           Brokeback Mountain was an AWESOME movie.      1\n",
      "1920  I like Mission Impossible movies because you n...      1\n",
      "3153  Derek and I saw 3 movies, Brokeback Mountain, ...      1\n"
     ]
    }
   ],
   "source": [
    "corpus = corpus.sample(frac=1)\n",
    "\n",
    "print(corpus.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El aprendizaje se va a dar a tráves de secuencias de palabras, es decir, de alguna forma tenemos que representar una secuencia de palabras por un vector de tamaño fijo.\n",
    "\n",
    "Ejemplo: Si tenemos el texto `\"i liked the Da Vinci Code a lot\"`, lo primero que tenemos que hacer es representar ese texto por un vector de palabras: `[\"i\", \"liked\", \"the\", \"da\", \"vinci\", \"code\", \"a\", \"lot\"]`. Observa que el vector vector de palabras está en minúsculas y no contiene caracteres especiales, sólo letras. Aún con esto la red neuronal no es capaz de procesar dichas palabras, por lo que tenemos que generar un diccionario de palabras y sustituir la palabra por su índice en el diccionario, entonces, si el diccionario es como `{\"a\": 20, \"code\": 100, ...}`, podemos generar el vector de palabras como: `[56, 28, 33, 17, 109, 0, 1502]`. Es decir, que cada palabra va a ser sustituida por su índice en el diccionario, el diccionario va a contener los índices de cada palabra diferente.\n",
    "\n",
    "Vamos a definir una función llamada `text_to_vec` que reciba un texto (una cadena de caracteres) y nos devuelva un vector de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'liked', 'the', 'da', 'vinci', 'code', 'a', 'lot']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def text_to_vec(text):\n",
    "    return re.sub(r\"[^a-z\\s]\", \"\", text.lower()).split()\n",
    "\n",
    "text_to_vec(\"I liked the Da Vinci Code a lot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a formar una lista que contenga los textos representados como vectores de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3479       [brokeback, mountain, was, an, awesome, movie]\n",
      "5535    [i, hate, harry, potter, that, daniel, wotshis...\n",
      "3539       [brokeback, mountain, was, an, awesome, movie]\n",
      "1920    [i, like, mission, impossible, movies, because...\n",
      "3153    [derek, and, i, saw, movies, brokeback, mounta...\n",
      "5386    [this, quiz, sucks, and, harry, potter, sucks,...\n",
      "2402    [i, am, going, to, start, reading, the, harry,...\n",
      "4209    [da, vinci, code, up, up, down, down, left, ri...\n",
      "3109                      [i, loved, brokeback, mountain]\n",
      "6978    [she, helped, me, bobbypin, my, insanely, cool...\n",
      "6491                   [brokeback, mountain, was, boring]\n",
      "6438    [she, helped, me, bobbypin, my, insanely, cool...\n",
      "5303    [harry, potter, dragged, draco, malfoy, s, tro...\n",
      "835                       [i, love, the, da, vinci, code]\n",
      "2658    [harry, potter, is, awesome, i, dont, care, if...\n",
      "5728    [is, it, just, me, or, does, harry, potter, suck]\n",
      "6182    [then, dinner, with, min, and, rosie, and, bro...\n",
      "6657    [then, snuck, into, brokeback, mountain, which...\n",
      "4820    [by, the, way, the, da, vinci, code, sucked, j...\n",
      "5120    [hopefully, this, is, not, the, case, and, the...\n",
      "894                        [da, vinci, code, is, awesome]\n",
      "4740    [by, the, way, the, da, vinci, code, sucked, j...\n",
      "18          [thing, is, i, enjoyed, the, da, vinci, code]\n",
      "3703    [anyway, thats, why, i, love, brokeback, mount...\n",
      "389     [the, people, who, are, worth, it, know, how, ...\n",
      "3577    [hes, likeyeah, i, got, acne, and, i, love, br...\n",
      "5908    [is, it, just, me, or, does, harry, potter, suck]\n",
      "106                   [the, da, vinci, code, is, awesome]\n",
      "2916    [because, i, would, like, to, make, friends, w...\n",
      "4386                         [da, vinci, code, sucks, be]\n",
      "                              ...                        \n",
      "2822    [i, am, going, to, start, reading, the, harry,...\n",
      "224                        [da, vinci, code, is, awesome]\n",
      "1569    [i, love, being, a, sentry, for, mission, impo...\n",
      "3979       [brokeback, mountain, was, an, awesome, movie]\n",
      "4834    [friday, hung, out, with, kelsie, and, we, wen...\n",
      "5939    [always, knows, what, i, want, not, guy, crazy...\n",
      "1898    [which, is, why, i, said, silent, hill, turned...\n",
      "5675    [i, hate, harry, potter, that, daniel, wotshis...\n",
      "941     [the, da, vinci, code, was, awesome, i, cant, ...\n",
      "4125                        [and, da, vinci, code, sucks]\n",
      "6120                         [brokeback, mountain, sucks]\n",
      "6610    [oh, and, brokeback, mountain, is, a, terrible...\n",
      "165     [i, saw, both, da, vinci, code, and, xmen, the...\n",
      "3829       [brokeback, mountain, was, an, awesome, movie]\n",
      "175              [hahash, i, love, da, vinci, code, tooo]\n",
      "3030    [then, back, to, my, house, to, watch, brokeba...\n",
      "1881                [mission, impossible, was, excellent]\n",
      "3213    [anyway, thats, why, i, love, brokeback, mount...\n",
      "1112                      [i, loved, mission, impossible]\n",
      "1760    [i, like, mission, impossible, movies, because...\n",
      "61                        [i, love, the, da, vinci, code]\n",
      "3551              [dudeee, i, loved, brokeback, mountain]\n",
      "2342    [i, am, going, to, start, reading, the, harry,...\n",
      "2238    [harry, potter, is, awesome, i, dont, care, if...\n",
      "189                    [i, luv, the, bookda, vinci, code]\n",
      "1777    [were, gonna, like, watch, mission, impossible...\n",
      "3425                       [i, love, brokeback, mountain]\n",
      "2011                             [i, love, harry, potter]\n",
      "4230    [by, the, way, the, da, vinci, code, sucked, j...\n",
      "5733    [harry, potter, dragged, draco, malfoy, s, tro...\n",
      "Name: text, Length: 7086, dtype: object\n"
     ]
    }
   ],
   "source": [
    "texts = corpus[\"text\"].map(text_to_vec)\n",
    "\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora necesitamos una lista que contenga todas las palabras de nuestros textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se encontraron 75401 palabras\n",
      "Primeras 100 palabras: ['brokeback', 'mountain', 'was', 'an', 'awesome', 'movie', 'i', 'hate', 'harry', 'potter', 'that', 'daniel', 'wotshisface', 'needs', 'a', 'fucking', 'slap', 'brokeback', 'mountain', 'was', 'an', 'awesome', 'movie', 'i', 'like', 'mission', 'impossible', 'movies', 'because', 'you', 'never', 'know', 'whos', 'on', 'the', 'right', 'side', 'derek', 'and', 'i', 'saw', 'movies', 'brokeback', 'mountain', 'which', 'was', 'beautiful', 'i', 'almost', 'cried', 'this', 'quiz', 'sucks', 'and', 'harry', 'potter', 'sucks', 'ok', 'bye', 'i', 'am', 'going', 'to', 'start', 'reading', 'the', 'harry', 'potter', 'series', 'again', 'because', 'that', 'is', 'one', 'awesome', 'story', 'da', 'vinci', 'code', 'up', 'up', 'down', 'down', 'left', 'right', 'left', 'right', 'b', 'a', 'suck', 'i', 'loved', 'brokeback', 'mountain', 'she', 'helped', 'me', 'bobbypin', 'my', 'insanely']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "\n",
    "for text_vec in texts:\n",
    "    words.extend(text_vec)\n",
    "    \n",
    "print(\"Se encontraron {} palabras\".format(len(words)))\n",
    "print(\"Primeras 100 palabras: {}\".format(words[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos una lista con todas las palabras en todos nuestros textos, por lo que formaremos un conjunto menor con las palabras direntes (las palabras únicas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se encontraron 2193 palabras diferentes\n",
      "Primeras 100: ['suspenseful', 'four', 'sucksi', 'asian', 'mcgarther', 'hating', 'genre', 'captain', 'hate', 'weekda', 'stinks', 'snowing', 'spec', 'created', 'walks', 'voted', 'scorebrokeback', 'trousers', 'lore', 'lord', 'sorry', 'worth', 'codeother', 'updated', 'oceans', 'betterwe', 'every', 'jack', 'bringing', 'tickets', 'school', 'prize', 'clothed', 'undercover', 'enjoy', 'jamaica', 'tired', 'warns', 'hanging', 'feathers', 'messy', 'second', 'street', 'friendships', 'n', 'even', 'hide', 'christ', 'musiclove', 'new', 'increasing', 'ever', 'disney', 'told', 'deemed', 'kicked', 'hero', 'zach', 'gayer', 'never', 'here', 'nanny', 'dork', 'kudos', 'study', 'changed', 'controversy', 'credit', 'dudeee', 'aka', 'postponed', 'changes', 'campaign', 'blashpemies', 'julia', 'lynne', 'bro', 'total', 'plot', 'spoke', 'would', 'geisha', 'music', 'preview', 'type', 'until', 'holy', 'oscar', 'haha', 'yahoo', 'award', 'disruption', 'adult', 'excellent', 'hold', 'must', 'me', 'enjoying', 'word', 'room']\n"
     ]
    }
   ],
   "source": [
    "unique_words = list(set(words))\n",
    "\n",
    "print(\"Se encontraron {} palabras diferentes\".format(len(unique_words)))\n",
    "print(\"Primeras 100: {}\".format(unique_words[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a definir dos funciones útiles, una llamada `word_frequency` que nos devolverá la frecuencia de una palabra y otra llamada `word_index` que nos devolverá el índice de la palabra en el diccionario de palabras únicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3221"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_frequency(word):\n",
    "    return words.count(word)\n",
    "\n",
    "word_frequency(\"the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_index(word):\n",
    "    if not word in unique_words:\n",
    "        return 0\n",
    "    return unique_words.index(word) + 1\n",
    "\n",
    "word_index(\"the\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opcionalmente podemos filtrar las palabras únicas que tengan un frecuancia mayor a un umbral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se encontraron 450 palabras diferentes\n"
     ]
    }
   ],
   "source": [
    "THRESH = 5\n",
    "\n",
    "unique_words = list(filter(lambda word: word_frequency(word) >= THRESH, unique_words))\n",
    "\n",
    "print(\"Se encontraron {} palabras diferentes\".format(len(unique_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que conocemos la frecuencia de una palabra en el corpus y su posición en el diccionario de palabras diferentes, procederemos a generar secuencias de palabras. Podemos definir el tamaño de la cuencia manualmente o utilizar el tamaño de secuencia de palabras más grande para nuestros textos. El tamaño de secuencia es el número de palabras en el texto.\n",
    "\n",
    "Obtenemos el número de palabras por texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaños de las secuencias para los primeros 100 textos: [6, 11, 6, 14, 13, 9, 17, 14, 4, 21, 4, 21, 32, 6, 11, 9, 14, 15, 12, 29, 5, 12, 8, 7, 15, 10, 9, 6, 34, 5, 6, 28, 4, 6, 8, 5, 21, 11, 5, 6, 30, 14, 7, 25, 10, 5, 7, 15, 15, 4, 21, 12, 11, 28, 14, 14, 4, 4, 7, 7, 6, 4, 7, 4, 22, 4, 6, 8, 12, 4, 6, 4, 12, 9, 12, 4, 8, 4, 6, 16, 21, 7, 7, 11, 5, 12, 4, 5, 13, 10, 21, 4, 25, 7, 5, 6, 6, 6, 8, 11]\n"
     ]
    }
   ],
   "source": [
    "texts_len = list(map(len, texts))\n",
    "# texts_len = list(map(lambda text_vec: len(text_vec), texts))\n",
    "\n",
    "print(\"Tamaños de las secuencias para los primeros 100 textos: {}\".format(texts_len[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la lista anterior podemos calcular cuál es la secuencia máxima de palabras, es decir, cuál es el texto que tiene más palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La máxima secuencia de palabras es: 38\n"
     ]
    }
   ],
   "source": [
    "max_seq = max(texts_len) # podemos sino utilizar un número fijo, ej. 100\n",
    "\n",
    "print(\"La máxima secuencia de palabras es: {}\".format(max_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con anterior podemos definir una función llamada `text_to_seq` que convierta un texto (en su forma de vector de palabras) en una secuencia de índices respecto al diccionario de palabras únicas.\n",
    "\n",
    "Ejemplo: Tenemos el texto `[\"i\", \"love\", \"brokeback\", \"mountain\"]` representado como un vector de palabras, la secuencia generada debería quedar como `[0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 2053,  632,  775, 1394]`. Observa que la secuencia contine los índices de cada palabra y rellena de ceros las demás posiciones para dejar una secuencia de tamaño fijo igual a `max_seq`, poniendo las palabras al final de la secuencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0, 427, 144, 170, 329])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def text_to_seq(text_vec):\n",
    "    seq = [0] * max_seq\n",
    "    n = len(text_vec)\n",
    "    seq[-n:] = list(map(word_index, text_vec))\n",
    "    return np.array(seq)\n",
    "    \n",
    "text_to_seq([\"i\", \"love\", \"brokeback\", \"mountain\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ya podemos convertir todos textos a secuencias, para formar la matriz de entrenamiento `x_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La primer secuencia quedaría como: [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0 170 329 127 418\n",
      " 150 314]\n"
     ]
    }
   ],
   "source": [
    "texts_seq = list(map(text_to_seq, texts))\n",
    "\n",
    "print(\"La primer secuencia quedaría como: {}\".format(texts_seq[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de las secuancias, podemos calcular `x_train` y `x_test` para realizar el aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se analizarán 6731/7086 secuencias\n",
      "Se validarán 355/7086 secuencias\n"
     ]
    }
   ],
   "source": [
    "n = len(texts_seq)\n",
    "k = int(0.95 * n)\n",
    "\n",
    "x_train = np.array(texts_seq[:k])\n",
    "x_test = np.array(texts_seq[k:])\n",
    "\n",
    "print(\"Se analizarán {}/{} secuencias\".format(len(x_train), n))\n",
    "print(\"Se validarán {}/{} secuencias\".format(len(x_test), n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras 3 secuencias para el entrenamiento\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0 170 329 127 418\n",
      "  150 314]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0 427   1 259 173 181  50 345 274 276\n",
      "  338 186]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0 170 329 127 418\n",
      "  150 314]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Primeras 3 secuencias para el entrenamiento\")\n",
    "print(x_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para formar `y_train` y `y_test` realizamos un proceso similar, utilizando las etiquetas del corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras 3 etiquetas para el entrenamiento:\n",
      "3479    1\n",
      "5535    0\n",
      "3539    1\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y_train = corpus[\"label\"][:k]\n",
    "y_test = corpus[\"label\"][k:]\n",
    "\n",
    "print(\"Primeras 3 etiquetas para el entrenamiento:\")\n",
    "print(y_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ya tenemos todos los elementos para entrenar nuestra red RNN-GRU (`x_train`, `x_test`, `y_train`, `y_test`).\n",
    "\n",
    "Vamos a crear una red neuronal recurrente RNN con unidad de memoria GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 38, 8)             3608      \n",
      "_________________________________________________________________\n",
      "gru_10 (GRU)                 (None, 38, 16)            1200      \n",
      "_________________________________________________________________\n",
      "gru_11 (GRU)                 (None, 38, 8)             600       \n",
      "_________________________________________________________________\n",
      "gru_12 (GRU)                 (None, 4)                 156       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 5,569\n",
      "Trainable params: 5,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, GRU, Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Defininimos la capa de transformación de índices a vectores\n",
    "EMBEDDING_SIZE = 8\n",
    "model.add(Embedding(\n",
    "    input_dim=len(unique_words) + 1,\n",
    "    output_dim=EMBEDDING_SIZE,\n",
    "    input_length=max_seq\n",
    "))\n",
    "\n",
    "# Definimos las capas GRU\n",
    "model.add(GRU(16, return_sequences=True))\n",
    "model.add(GRU(8, return_sequences=True))\n",
    "model.add(GRU(4))\n",
    "\n",
    "# Definimos la capa Densa para el entrenamiento\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces procedemos a entrenar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6731/6731 [==============================] - 24s 4ms/step - loss: 0.3866 - acc: 0.8214\n",
      "Epoch 2/10\n",
      "6731/6731 [==============================] - 20s 3ms/step - loss: 0.0902 - acc: 0.9816\n",
      "Epoch 3/10\n",
      "6731/6731 [==============================] - 21s 3ms/step - loss: 0.0467 - acc: 0.9902\n",
      "Epoch 4/10\n",
      "6731/6731 [==============================] - 21s 3ms/step - loss: 0.0296 - acc: 0.9941\n",
      "Epoch 5/10\n",
      "6731/6731 [==============================] - 21s 3ms/step - loss: 0.0229 - acc: 0.9957\n",
      "Epoch 6/10\n",
      "6731/6731 [==============================] - 21s 3ms/step - loss: 0.0182 - acc: 0.9970\n",
      "Epoch 7/10\n",
      "6731/6731 [==============================] - 21s 3ms/step - loss: 0.0178 - acc: 0.9958\n",
      "Epoch 8/10\n",
      "6731/6731 [==============================] - 21s 3ms/step - loss: 0.0119 - acc: 0.9976\n",
      "Epoch 9/10\n",
      "6731/6731 [==============================] - 21s 3ms/step - loss: 0.0103 - acc: 0.9984\n",
      "Epoch 10/10\n",
      "6731/6731 [==============================] - 20s 3ms/step - loss: 0.0088 - acc: 0.9987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12545fb90>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train.values, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355/355 [==============================] - 1s 3ms/step\n",
      "[0.04572702128584431, 0.9859154929577465]\n"
     ]
    }
   ],
   "source": [
    "metrics = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora con la red RNN entrenada podemos pensar en un texto (en inglés) para predecir si va a ser una reseña positiva o negativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(text):\n",
    "    text_vec = text_to_vec(text)\n",
    "    text_seq = text_to_seq(text_vec)\n",
    "    predict = model.predict(text_seq.reshape(1, -1))\n",
    "    return predict[0][0]\n",
    "\n",
    "def sentiment_positive(text):\n",
    "    return sentiment(text) >= 0.5\n",
    "\n",
    "def sentiment_negative(text):\n",
    "    return sentiment(text) < 0.5\n",
    "\n",
    "def sentiment_label(text):\n",
    "    return \"POSITIVE\" if sentiment_positive(text) else \"NEGATIVE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "text = \"I hate you Harry Potter\"\n",
    "\n",
    "print(sentiment_label(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE\n"
     ]
    }
   ],
   "source": [
    "text = \"I love you Harry Potter\"\n",
    "\n",
    "print(sentiment_label(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "text = \"Impossible Mission\"\n",
    "\n",
    "print(sentiment_label(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
